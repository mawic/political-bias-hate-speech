{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "from helper.data_loading import *\n",
    "from helper.preprocessing import *\n",
    "from helper.cross_val_model import *\n",
    "from helper.classifier_helper import *\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germEval = pd.read_pickle('data/GERMEVAL_with_topic_distribution.pkl')\n",
    "germEval = germEval.rename(columns={'label_1': 'label'})\n",
    "\n",
    "germEval_topic_distribution = get_topic_distribution_over_dataset(germEval.loc[germEval.label==\"OTHER\",:])\n",
    "print(\"germEval topic distribution:\\n\",germEval_topic_distribution, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    print(\"start remove_hand_selected_words\")\n",
    "    data[\"token\"] = data.apply(lambda x: remove_hand_selected_words(x[\"text\"]), axis=1)\n",
    "    \n",
    "    print(\"start rermove_repeating_chars\")\n",
    "    data[\"token\"] = data.apply(lambda x: rermove_repeating_chars(x[\"token\"]), axis=1)\n",
    "    \n",
    "    print(\"start emoji_2_text\")\n",
    "    data[\"token\"] = data.apply(lambda x: emoji_2_text(x[\"token\"]), axis=1)\n",
    "    \n",
    "    print(\"start ekphrasis\")\n",
    "    data[\"token\"] = data.apply(lambda x: \" \".join(tw_process.pre_process_doc(x[\"token\"])), axis=1)\n",
    "    \n",
    "    print(\"start remove_special_chars\")\n",
    "    data[\"token\"] = data.apply(lambda x: remove_special_chars(x[\"token\"]), axis=1) #[r\"[^A-Za-z0-9\\säüßöÖÄÜ<>_:!?.,\\-]+\n",
    "          \n",
    "    print(\"start lower\")\n",
    "    data[\"token\"] = data.apply(lambda x: x[\"token\"].lower(), axis=1) \n",
    "    \n",
    "    print(\"start sentence_to_token\")\n",
    "    data[\"token\"] = data.apply(lambda x: sentence_to_token(x[\"token\"]), axis=1)\n",
    "\n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.read_pickle('data/LEFT_with_topic_distribution.pkl')\n",
    "left_pool = create_sample_pool(left, germEval_topic_distribution, sample_factor = 5)\n",
    "\n",
    "left = create_data_from_pool(left_pool, germEval_topic_distribution)    \n",
    "left = concat_germEval_and_Other(germEval, left)\n",
    "left = prepare_data(left)    \n",
    "left = replace_label_to_binary(left)\n",
    "\n",
    "del left_pool\n",
    "\n",
    "tweets = np.array(left[\"token\"].tolist())\n",
    "labels = np.array(left[\"label\"].tolist())\n",
    "\n",
    "scores_model = perform_cross_validation(folds=3,\n",
    "                                 tweets=tweets,\n",
    "                                 labels=labels,\n",
    "                                 print_fold_eval=True)\n",
    "del left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = pd.read_pickle('data/RIGHT_with_topic_distribution.pkl')\n",
    "right_pool = create_sample_pool(right, germEval_topic_distribution, sample_factor = 5)\n",
    "\n",
    "right = create_data_from_pool(right_pool, germEval_topic_distribution)    \n",
    "right = concat_germEval_and_Other(germEval, right)\n",
    "right = prepare_data(right)    \n",
    "right = replace_label_to_binary(right)\n",
    "\n",
    "del right_pool\n",
    "\n",
    "tweets = np.array(right[\"token\"].tolist())\n",
    "labels = np.array(right[\"label\"].tolist())\n",
    "\n",
    "scores_model = perform_cross_validation(folds=3,\n",
    "                                 tweets=tweets,\n",
    "                                 labels=labels,\n",
    "                                 print_fold_eval=True)\n",
    "\n",
    "del right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral = pd.read_pickle('data/NEUTRAL_with_topic_distribution.pkl')\n",
    "neutral_pool = create_sample_pool(neutral, germEval_topic_distribution, sample_factor = 5)\n",
    "\n",
    "neutral = create_data_from_pool(neutral_pool, germEval_topic_distribution)    \n",
    "neutral = concat_germEval_and_Other(germEval, neutral)\n",
    "neutral = prepare_data(neutral)    \n",
    "neutral = replace_label_to_binary(neutral)\n",
    "\n",
    "del neutral_pool\n",
    "\n",
    "tweets = np.array(neutral[\"token\"].tolist())\n",
    "labels = np.array(neutral[\"label\"].tolist())\n",
    "\n",
    "scores_model = perform_cross_validation(folds=3,\n",
    "                                 tweets=tweets,\n",
    "                                 labels=labels,\n",
    "                                 print_fold_eval=True)\n",
    "\n",
    "del neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germEval = prepare_data(germEval)    \n",
    "germEval = replace_label_to_binary(germEval)\n",
    "\n",
    "tweets = np.array(germEval[\"token\"].tolist())\n",
    "labels = np.array(germEval[\"label\"].tolist())\n",
    "\n",
    "scores_model = perform_cross_validation(folds=3,\n",
    "                                 tweets=tweets,\n",
    "                                 labels=labels,\n",
    "                                 print_fold_eval=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germEval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germEval = pd.read_pickle('data/GERMEVAL_with_topic_distribution.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germEval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germEval.groupby(\"label_1\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germEval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset germEval 2019\n",
    "#emojies are not decoded properly\n",
    "germeval2019_subtask1_2_train = pd.read_csv('data/germEval2019/germeval2019.training_subtask1_2_korrigiert.txt',\n",
    "            sep = \"\\t\",encoding=\"utf-8\",quoting=csv.QUOTE_NONE ,\n",
    "            names=['text','label_1','label_2'])\n",
    "\n",
    "germeval2019_subtask1_2_train[\"origin\"] = \"train\"\n",
    "germeval2019_subtask1_2_train[\"year\"] = \"19\"\n",
    "\n",
    "#emojies are not decoded properly\n",
    "germeval2019_subtask1_2_test = pd.read_csv('data/germEval2019/germeval2019GoldLabelsSubtask1_2.txt',\n",
    "            sep = \"\\t\",encoding=\"utf-8\",quoting=csv.QUOTE_NONE ,\n",
    "            names=['text','label_1','label_2'])\n",
    "\n",
    "germeval2019_subtask1_2_test[\"origin\"] = \"test\"\n",
    "germeval2019_subtask1_2_test[\"year\"] = \"19\"\n",
    "\n",
    "#load dataset germEval 2018\n",
    "germEval2018_train = pd.read_csv('data/germEval2018/germeval2018.training.txt',\n",
    "            sep = \"\\t\",encoding=\"utf-8\",quoting=csv.QUOTE_NONE ,\n",
    "            names=['text','label_1','label_2'])\n",
    "\n",
    "germEval2018_train[\"origin\"] = \"train\"\n",
    "germEval2018_train[\"year\"] = \"18\"\n",
    "\n",
    "germEval2018_test = pd.read_csv('data/germEval2018/germeval2018.test.txt',\n",
    "            sep = \"\\t\",encoding=\"utf-8\",quoting=csv.QUOTE_NONE ,\n",
    "            names=['text','label_1','label_2'])\n",
    "\n",
    "germEval2018_test[\"origin\"] = \"test\"\n",
    "germEval2018_test[\"year\"] = \"18\"\n",
    "\n",
    "df = pd.concat([germeval2019_subtask1_2_train,\n",
    "           germeval2019_subtask1_2_test,\n",
    "           germEval2018_train,\n",
    "           germEval2018_test])\n",
    "\n",
    "#remove duplicate tweets, due concatinating different datasets together\n",
    "df = df.drop_duplicates()\n",
    "df = df.sample(frac=1,random_state=1993).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"label_1\",\"origin\",\"year\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"year\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1688\t1287\n",
    "1202\t970\n",
    "3321\t2708\n",
    "2330\t2061\n",
    "\n",
    "\n",
    "8541\t7026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1688+1287+1202+970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3321+2708+2330+2061"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}